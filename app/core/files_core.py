import os
from fastapi import APIRouter, HTTPException, Body, Query, UploadFile, File, Request
from fastapi.responses import JSONResponse, StreamingResponse
from bson import ObjectId
from datetime import datetime
from pathlib import Path
from typing import List, Optional
from app.databases.mongo_connector import mongo
from app.config import muse_config
from app.core.utils import chunk_file, ensure_list
from app.core.memory_core import log_message

collection_name = muse_config.get("MONGO_PROJECTS_COLLECTION")
MONGO_PROJECTS_COLLECTION = muse_config.get("MONGO_PROJECTS_COLLECTION")
MONGO_FILES_COLLECTION = muse_config.get("MONGO_FILES_COLLECTION")
MONGO_FACTS_COLLECTION = "muse_cortex"
MONGO_MESSAGES_COLLECTION = "muse_conversations"
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
FILES_DIR = PROJECT_ROOT / "uploadedfiles"

def ensure_files_dir():
    if not os.path.exists(FILES_DIR):
        os.makedirs(FILES_DIR, exist_ok=True)

async def chunk_and_store_text(file, file_id, project_ids: Optional[List[str]]):
    chunks = chunk_file(file)
    message_ids = []
    for chunk in chunks:
        log_entry = {
            "role": "system",
            "message": chunk["content"],
            "source": "file",
            "metadata": {
                "filename": file.filename,
                "mimetype": file.content_type,
                "file_id": file_id,
                "file_index": chunk["index"],
                "start_line": chunk["start_line"],
                "end_line": chunk["end_line"],
                "start_byte": chunk["start_byte"],
                "end_byte": chunk["end_byte"]
            },
            "project_ids": project_ids  # May be empty
        }
        result = await log_message(**log_entry)
        msg_id = result.get("message_id")
        message_ids.append(msg_id)
    return message_ids


async def handle_text_file_upload(project_ids: Optional[List[str]], file: UploadFile):
    ensure_files_dir()
    file_bytes = await file.read()
    if not file_bytes:
        raise ValueError("No file data received.")

    new_file_id = ObjectId()
    file_path = os.path.join(FILES_DIR, str(new_file_id))
    with open(file_path, "wb") as f:
        f.write(file_bytes)

    # Prepare project ObjectIds list (may be empty)
    project_obj_ids = [ObjectId(pid) for pid in project_ids] if project_ids else []

    # Chunk the file and log each chunk as a message
    chunks = chunk_file(file_bytes)
    message_ids = []
    for chunk in chunks:
        log_entry = {
            "role": "system",
            "message": chunk["content"],
            "source": "file",
            "metadata": {
                "filename": file.filename,
                "mimetype": file.content_type,
                "file_id": new_file_id,
                "file_index": chunk["index"],
                "start_line": chunk["start_line"],
                "end_line": chunk["end_line"],
                "start_byte": chunk["start_byte"],
                "end_byte": chunk["end_byte"]
            },
            "project_ids": project_obj_ids  # May be empty
        }
        result = await log_message(**log_entry)
        msg_id = result.get("message_id")
        message_ids.append(msg_id)

    file_doc = {
        "_id": new_file_id,
        "filename": file.filename,
        "mimetype": file.content_type,
        "size": len(file_bytes),
        "path": file_path,
        "message_ids": message_ids,
        "project_ids": project_obj_ids,
        "uploaded_on": datetime.utcnow(),
    }
    mongo.insert_one_document(MONGO_FILES_COLLECTION, file_doc)

    # For each project, add file_id to its file_ids array
    for pid in project_obj_ids:
        mongo.update_one_document_array(
            MONGO_PROJECTS_COLLECTION,
            {"_id": pid},
            {"$push": {"file_ids": new_file_id}}
        )

    return {
        "message": f"File '{file.filename}' uploaded and chunked.",
        "num_chunks": len(chunks),
        "message_ids": [str(mid) for mid in message_ids],
        "file_id": str(new_file_id),
        "path": file_path
    }

async def handle_image_file_upload(project_ids: Optional[List[str]], file: UploadFile):
    ensure_files_dir()
    file_bytes = await file.read()
    if not file_bytes:
        raise ValueError("No file data received.")

    new_file_id = ObjectId()
    new_fact_id = ObjectId()
    filename = file.filename
    mimetype = file.content_type
    _, ext = os.path.splitext(filename)
    file_path = os.path.join(FILES_DIR, str(new_file_id) + ext)

    with open(file_path, "wb") as f:
        f.write(file_bytes)

    project_obj_ids = [ObjectId(pid) for pid in project_ids] if project_ids else []



    # --- Auto-caption step ---
    from app.services.openai_client import get_openai_image_caption_bytes
    caption = get_openai_image_caption_bytes(
        file_bytes,
        #prompt="Describe this image in one clear, informative sentence for a project file caption.",
        prompt="Describe this image with a clear, detailed paragraph that would allow an LLM to understand the contents.",
        mime_type=mimetype or "image/jpeg"
    )

    file_doc = {
        "_id": new_file_id,
        "filename": filename,
        "mimetype": mimetype,
        "size": len(file_bytes),
        "path": file_path,
        "project_ids": project_obj_ids,
        "fact_id": new_fact_id,
        "uploaded_on": datetime.utcnow(),
        "caption": caption,
    }
    mongo.insert_one_document(MONGO_FILES_COLLECTION, file_doc)

    cortex_doc = {
        "_id": new_fact_id,
        "type": "fact",
        "file_id": new_file_id,
        "project_ids": project_obj_ids,
        "filename": filename,
        "text": caption,
        "mimetype": mimetype,
        "created_at": datetime.utcnow(),
    }
    mongo.insert_one_document("muse_cortex", cortex_doc)

    # Link file to all specified projects
    for pid in project_obj_ids:
        mongo.update_one_document_array(
            MONGO_PROJECTS_COLLECTION,
            {"_id": pid},
            {"$push": {"file_ids": new_file_id}}
        )

    return {
        "message": f"Image '{filename}' uploaded and captioned.",
        "file_id": str(new_file_id),
        "caption": caption,
        "path": file_path
    }

async def modify_file_project_link_core(
    project_id: str,
    file_id: str,
    action: str,  # "attach" or "detach"
    *,
    mongo,
    index_queue
):
    # Choose mongo operator
    op = "$addToSet" if action == "attach" else "$pull"

    # 1. Project â‡„ File
    mongo.update_one_document_array(
        collection_name=MONGO_PROJECTS_COLLECTION,
        filter_query={"_id": ObjectId(project_id)},
        update_data={op: {"file_ids": ObjectId(file_id)}}
    )
    mongo.update_one_document_array(
        collection_name=MONGO_FILES_COLLECTION,
        filter_query={"_id": ObjectId(file_id)},
        update_data={op: {"project_ids": ObjectId(project_id)}}
    )

    # 2. Facts/Messages (recursive link)
    file_doc = mongo.find_one_document(MONGO_FILES_COLLECTION, {"_id": ObjectId(file_id)})
    for key in ("fact_id", "message_ids"):
        ids = file_doc.get(key)
        id_list = ensure_list(ids)
        for idx in id_list:
            if key == "fact_id":
                collection = MONGO_FACTS_COLLECTION
                filter_query = {"_id": ObjectId(idx)}
            else:  # key == "message_ids"
                collection = MONGO_MESSAGES_COLLECTION
                filter_query = {"message_id": idx}  # <-- Use message_id as primary key
            # Include your update fields here (with updated_on, etc)
            update_fields = {
                op: {"project_ids": ObjectId(project_id)},
                "$set": {"updated_on": datetime.utcnow()}
            }
            mongo.update_one_document_array(
                collection_name=collection,
                filter_query=filter_query,
                update_data=update_fields
            )
            # Re-index if message
            if collection == MONGO_MESSAGES_COLLECTION:
                await index_queue.put(idx)

    return { "ok": True, "action": action, "indexing_queued": True }

def get_all_message_ids_for_files(file_ids):
    """
    Given a list of file_ids (ObjectIds), return a deduplicated list of all message_ids
    from those files. Skips files with missing or empty message_ids.
    """
    # Fetch all file docs in one go
    files = mongo.find_documents(
        collection_name=MONGO_FILES_COLLECTION,
        query={"_id": {"$in": file_ids}}
    )
    # Collect all message_ids, flatten, and dedupe
    all_message_ids = set()
    for file in files:
        mids = file.get("message_ids") or []
        all_message_ids.update(mids)
    return list(all_message_ids)